# MindGen LLM Fallback Configuration

**Version:** v0.1.0  
**Date:** December 2024  
**Status:** Implemented

## üìã Overview

MindGen now features a configurable LLM fallback system that allows for flexible and robust agent initialization. The fallback logic is no longer hardcoded in agent files but is centrally configured and managed.

## üîß Configuration

### System Configuration (`config/system.yaml`)

```yaml
# LLM Fallback Configuration
llm_fallback:
  # Primary LLM for agents
  primary: "personal_chatgpt"
  # Fallback sequence if primary fails
  fallback_sequence: ["qwen_fallback", "deepseek"]
  # Specific fallback model for Qwen (Alibaba Cloud)
  qwen_fallback_model: "qwen3-235b-a22b"
```

### Model Configuration (`config/models.yaml`)

```yaml
# Qwen Fallback Model (Alibaba Cloud) - for agent fallback
qwen_fallback:
  api_url: "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"
  api_key: "${QWEN_API_KEY}"
  model_name: "qwen3-235b-a22b"
  max_tokens: 16000
  temperature: 0.7
  timeout: 45
  retry_attempts: 3
  retry_delay: 2
  rate_limit: 100
```

## üöÄ Fallback Logic

### Priority Sequence

1. **Primary**: `personal_chatgpt` (Personal ChatGPT Server)
2. **Fallback 1**: `qwen_fallback` (Qwen3-235B-A22B from Alibaba Cloud)
3. **Fallback 2**: `deepseek` (DeepSeek Reasoner)

### Implementation

The fallback logic is implemented in `services/llm_service.py`:

```python
def get_agent_llm(self) -> Any:
    """Get the appropriate LLM for agents based on fallback configuration"""
    try:
        # Get fallback configuration
        fallback_config = self.config_manager.get_system_config().get('llm_fallback', {})
        primary_model = fallback_config.get('primary', 'personal_chatgpt')
        fallback_sequence = fallback_config.get('fallback_sequence', ['qwen_fallback', 'deepseek'])
        
        # Try primary model first
        if primary_model in self.llms and self.llms[primary_model] is not None:
            logger.info(f"Using primary LLM: {primary_model}")
            return self.llms[primary_model]
        
        # Try fallback sequence
        for fallback_model in fallback_sequence:
            if fallback_model in self.llms and self.llms[fallback_model] is not None:
                logger.info(f"Using fallback LLM: {fallback_model}")
                return self.llms[fallback_model]
        
        # If no fallback available, raise exception
        raise Exception("No LLM available for agent initialization")
        
    except Exception as e:
        logger.error(f"Error getting agent LLM: {str(e)}")
        raise
```

## üîÑ Agent Integration

All agent files now use the configurable fallback system:

### Before (Hardcoded)
```python
# Create agent with fallback LLM
agent_llm = self.llm_service.llms['personal_chatgpt']
if agent_llm is None:
    # Fallback to DeepSeek if personal_chatgpt is not available
    agent_llm = self.llm_service.llms['deepseek']
if agent_llm is None:
    # Fallback to Qwen if DeepSeek is also not available
    agent_llm = self.llm_service.llms['qwen']
```

### After (Configurable)
```python
# Create agent with configurable fallback LLM
agent_llm = self.llm_service.get_agent_llm()
```

## üõ†Ô∏è Customization

### Changing Primary LLM

To change the primary LLM, modify `config/system.yaml`:

```yaml
llm_fallback:
  primary: "deepseek"  # Change from "personal_chatgpt" to "deepseek"
```

### Modifying Fallback Sequence

To change the fallback order:

```yaml
llm_fallback:
  fallback_sequence: ["deepseek", "qwen_fallback"]  # DeepSeek first, then Qwen
```

### Adding New Fallback Models

1. Add the model configuration to `config/models.yaml`
2. Add the model to the fallback sequence in `config/system.yaml`
3. Ensure the model is initialized in `services/llm_service.py`

## üìä Benefits

1. **Configurable**: No hardcoded fallback logic in agent files
2. **Flexible**: Easy to change primary and fallback models
3. **Robust**: Multiple fallback options ensure availability
4. **Maintainable**: Centralized configuration management
5. **Loggable**: Clear logging of which LLM is being used

## üîç Monitoring

The system logs which LLM is being used:

```
2025-08-01 16:39:12,769 - services.llm_service - INFO - get_agent_llm:382 - Using fallback LLM: qwen_fallback
```

## üéØ Use Cases

### China Users (Primary Use Case)
- **Primary**: Personal ChatGPT Server
- **Fallback**: Qwen3-235B-A22B (Alibaba Cloud)
- **Final Fallback**: DeepSeek

### International Users
- **Primary**: Personal ChatGPT Server
- **Fallback**: DeepSeek
- **Final Fallback**: Qwen

### Development/Testing
- **Primary**: Any available model
- **Fallback**: Any other available model
- **Graceful degradation**: Clear error messages if no models available

## üîß Technical Details

### QwenLLM Compatibility

The `QwenLLM` class was enhanced to support LangChain's function calling interface:

```python
def bind(self, **kwargs):
    """Bind additional parameters to the LLM (required for function calling)"""
    # Create a new instance with bound parameters
    bound_llm = QwenLLM(
        model_name=self.model_name,
        temperature=self.temperature,
        max_tokens=self.max_tokens,
        timeout=self.timeout
    )
    # Store bound parameters for later use
    bound_llm._bound_kwargs = kwargs
    return bound_llm

def __call__(self, messages, **kwargs):
    """Call the LLM with messages (required for function calling)"""
    # Convert messages to prompt
    if isinstance(messages, list):
        # Extract content from messages
        prompt = "\n".join([msg.get('content', '') for msg in messages if msg.get('content')])
    else:
        prompt = str(messages)
    
    # Call the invoke method
    return self.invoke(prompt)
```

## ‚úÖ Status

- ‚úÖ Configurable fallback system implemented
- ‚úÖ Qwen3-235B-A22B fallback model added
- ‚úÖ All agent files updated to use configurable fallback
- ‚úÖ QwenLLM compatibility with LangChain function calling
- ‚úÖ Comprehensive logging and error handling
- ‚úÖ Documentation updated

## üöÄ Next Steps

1. **Environment Configuration**: Set up proper environment variables for production
2. **Testing**: Test with actual API keys and different fallback scenarios
3. **Monitoring**: Add metrics for fallback usage and performance
4. **Documentation**: Update user guides with fallback configuration examples 